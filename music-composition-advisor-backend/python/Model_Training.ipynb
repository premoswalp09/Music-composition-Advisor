{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1b1fcd51ac2996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:25.283338Z",
     "start_time": "2024-11-09T10:26:22.089190Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9b5933ec39ab16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:25.315259Z",
     "start_time": "2024-11-09T10:26:25.287341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Final_cleaned_genres_output.csv\")\n",
    "\n",
    "# Keep only the necessary columns\n",
    "data = data[['CleanedGenre', 'Lyrics', 'Label']]\n",
    "\n",
    "data = data.dropna(subset=['CleanedGenre', 'Lyrics'])\n",
    "\n",
    "label_mapping = {'Positive': 0, 'Negative': 1, 'Neutral': 2}\n",
    "data['label'] = data['Label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3117a9b586fa252e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:27.995272Z",
     "start_time": "2024-11-09T10:26:25.362796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "train_genres, test_genres, train_lyrics, test_lyrics, train_labels, test_labels = train_test_split(  \n",
    "    data['CleanedGenre'], data['Lyrics'], data['label'],   \n",
    "    test_size=0.2, random_state=42  \n",
    ") \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_pair(genres, lyrics):  \n",
    "    encodings = tokenizer(  \n",
    "        list(genres),  \n",
    "        list(lyrics),  \n",
    "        padding=True,  \n",
    "        truncation=True,  \n",
    "        max_length=512,  \n",
    "        return_token_type_ids=True \n",
    "    )  \n",
    "    return encodings  \n",
    "\n",
    "train_encodings = encode_pair(train_genres, train_lyrics)  \n",
    "test_encodings = encode_pair(test_genres, test_lyrics)  \n",
    "\n",
    "train_dataset = Dataset.from_dict({  \n",
    "    'input_ids': train_encodings['input_ids'],  \n",
    "    'attention_mask': train_encodings['attention_mask'],  \n",
    "    'token_type_ids': train_encodings['token_type_ids'],  \n",
    "    'label': train_labels  \n",
    "})  \n",
    "\n",
    "test_dataset = Dataset.from_dict({  \n",
    "    'input_ids': test_encodings['input_ids'],  \n",
    "    'attention_mask': test_encodings['attention_mask'],  \n",
    "    'token_type_ids': test_encodings['token_type_ids'],  \n",
    "    'label': test_labels  \n",
    "})  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdb84fe80cd7e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:31.864745Z",
     "start_time": "2024-11-09T10:26:29.725723Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights for balanced training\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(zip(range(len(class_weights)), class_weights))\n",
    "\n",
    "# Convert to tensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=3, problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd4286e6b61bb77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:35.228214Z",
     "start_time": "2024-11-09T10:26:35.208674Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss with class weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4643209c5252e425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:42.408489Z",
     "start_time": "2024-11-09T10:26:42.355419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ved\\anaconda3\\envs\\AI_F2_torch\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=16,  \n",
    "    max_steps=200,                    \n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=20,                  # Reduced warmup steps (10% of max_steps)  \n",
    "    weight_decay=0.01,  \n",
    "    logging_dir='./logs',  \n",
    "    logging_steps=10,  \n",
    "    evaluation_strategy=\"steps\",      \n",
    "    eval_steps=50,                    # Evaluate every 50 steps  \n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=50,                    # Save every 50 steps  \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"eval_loss\",  \n",
    "    greater_is_better=False,  \n",
    "    learning_rate=2e-5,  \n",
    "    lr_scheduler_type=\"cosine\"  \n",
    "\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a487670418a5e149",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:37:06.660830Z",
     "start_time": "2024-11-09T10:26:44.099586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "D:\\Users\\ved\\anaconda3\\envs\\AI_F2_torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 10:13, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.687404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.562221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>0.603095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.673572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.676328970193863, metrics={'train_runtime': 621.5622, 'train_samples_per_second': 5.148, 'train_steps_per_second': 0.322, 'total_flos': 827228585336832.0, 'train_loss': 0.676328970193863, 'epoch': 4.651162790697675})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ac8a122738511f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:39:14.459283Z",
     "start_time": "2024-11-09T10:38:59.250247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5622211694717407, 'eval_runtime': 15.1922, 'eval_samples_per_second': 11.124, 'eval_steps_per_second': 0.724, 'epoch': 4.651162790697675}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715304339e1c5785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:39:32.359260Z",
     "start_time": "2024-11-09T10:39:31.382687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/content/mymodel2\\\\tokenizer_config.json',\n",
       " './results/content/mymodel2\\\\special_tokens_map.json',\n",
       " './results/content/mymodel2\\\\vocab.txt',\n",
       " './results/content/mymodel2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "output_dir = './results/content/mymodel2'\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b902da3eba7fcdf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:39:39.783686Z",
     "start_time": "2024-11-09T10:39:39.780077Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(genre, lyrics, model, tokenizer):  \n",
    "    # Tokenize genre and lyrics as separate inputs  \n",
    "    inputs = tokenizer(  \n",
    "        genre,  \n",
    "        lyrics,  \n",
    "        return_tensors=\"pt\",  \n",
    "        padding=True,  \n",
    "        truncation=True,  \n",
    "        max_length=512,  \n",
    "        return_token_type_ids=True  # Important for distinguishing genre and lyrics  \n",
    "    )  \n",
    "    with torch.no_grad():  \n",
    "        outputs = model(**inputs)  \n",
    "\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()  \n",
    "\n",
    "    label_mapping = {0: \"Positive\", 1: \"Negative\", 2: \"Neutral\"}  \n",
    "\n",
    "    predicted_label = label_mapping[predicted_class]  \n",
    "    return predicted_label  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9bbd1fb668bbe98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T12:41:42.337814Z",
     "start_time": "2024-11-09T12:41:41.509841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('results/content/mymodel2')  # or the path where your fine-tuned model is saved\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input for testing (example)\n",
    "genre_input = \"Pop\"  # Example genre\n",
    "# lyrics_input = \"I've been reading books of old\tThe legends and the myths\tAchilles and his gold\tHercules and his gifts\tSpiderman's control\tAnd Batman with his fists\tAnd clearly I don't see myself upon that listBut she said, where'd you wanna go?\tHow much you wanna risk?\tI'm not looking for somebody\tWith some superhuman gifts\tSome superhero\tSome fairytale bliss\tJust something I can turn to\tSomebody I can kissI want something just like this\tDoo-doo-doo, doo-doo-doo\tDoo-doo-doo, doo-doo\tDoo-doo-doo, doo-doo-doo\tOh, I want something just like this\tDoo-doo-doo, doo-doo-doo\tDoo-doo-doo, doo-doo\tDoo-doo-doo, doo-doo-dooOh, I want something just like this\tI want something just like thisI've been reading books of old\tThe legends and the myths\tThe testaments they told\tThe moon and its eclipse\tAnd Superman unrolls\tA suit before he lifts\tBut I'm not the kind of person that it fitsShe said, where'd you wanna go?\tHow much you wanna risk?\tI'm not looking for somebody\tWith some superhuman gifts\tSome superhero\tSome fairytale bliss\tJust something I can turn to\tSomebody I can missI want something just like this\tI want something just like thisOh, I want something just like this\tDoo-doo-doo, doo-doo-doo\tDoo-doo-doo, doo-doo\tDoo-doo-doo, doo-doo-doo\tOh, I want something just like this\tDoo-doo-doo, doo-doo-doo\tDoo-doo-doo, doo-doo\tDoo-doo-doo, doo-doo-dooWhere'd you wanna go?\tHow much you wanna risk?\tI'm not looking for somebody\tWith some superhuman gifts\tSome superhero\tSome fairytale bliss\tJust something I can turn to\tSomebody I can kiss\tI want something just like thisOh, I want something just like this\tOh, I want something just like this\tOh, I want something just like this\"   # Example lyrics\n",
    "\n",
    "lyrics_input =\"no converse fake part bitches independent bitches part want paper part bitches flavored part part part part Ayy! part Bang shit hood one time Lil bitch back popping Tell ugly bitch move away need options Broke? fix pockets profit Quarter million switching part Bet bitch move old part 405 gun part Ayy still trona make plate Rich poor night choose fate Style top style night Five years rich night Drove Beamers Fig night Pushed Porsches Broadway dogging different hoes night Got chain worth Rolls night Got engine back top Nigga driving like bomb no converse fake part bitches independent bitches part want paper part bitches flavored part part part part Okay okay okay okay okay okay (That part Beggars cannot choosers bitch not Chipotle Nigga attitude feel like O'Shea Walkin' living legend man feel like Kobe left strip club got glitter Wifey going kill female OJ not feel man not okay Four Seasons take shower new clothes reloaded Rich night still eating catfish bitch not really bad catfish walk Saks Fifth paparazzi backflows lay mattress Blow back til backless Thick already established got done Yeah! Okay okay okay okay (That part Beggars cannot choosers bitch not Chipotle (That part Nigga attitude feel like O'Shea (That part Walkin' living legend man feel like Kobe (That part no converse fake part bitches independent bitches part want paper part bitches flavored part part part part Ayy! part Bang shit hood one time Lil bitch back popping Tell ugly bitch move away need options Broke? fix pockets profit million made still not part girl got matching part get slowed lose part XO thing go straight Need bitch go ways Style top style night Since lounging wanted ball night pistol drawls night broke sauce night Got Chevy side side Hundred spokes data dates Got chopper stand put homies beside no converse fake part bitches independent bitches part want paper part bitches flavored part part part part Ayy! part part part Walkin' living legend man feel like Kobe (That part dropped 60 man feel like Kobe Lamar man feel like Kobe Pippen wedding man feel like Jordan Trippin' wedding not say shit night listening close though listening hoes though would not listen flow though Listen Goat Listen young night 'Go though freestyle knew nights night ScHoolboy Q uh Top Dawg call Top Dawg Get night phone Top Dawg phone Ayy Hah!\"\n",
    "# Predict the sentiment label\n",
    "predicted_label = predict_sentiment(genre_input, lyrics_input, model, tokenizer)\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
